{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23918f4",
   "metadata": {},
   "source": [
    "This file takes a while to run since it goes through each link, loads it and scrapes the necessary data. Since it takes a while I've added in some code which takes any previously fetched movie data and ensures those are not scraped again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608f7bb8-4ab6-4d84-b742-d120705567b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links 30\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "MOVIE_LINKS_FILE = '2links24-12-2021-19-29-31-9457.pkl'\n",
    "PREV_MOVIES_DATA_FILE = \"test-1769.pkl\"\n",
    "\n",
    "test_links = ['https://www.rottentomatoes.com/m/rumble_2021',\n",
    " 'https://www.rottentomatoes.com/m/hurt_2021',\n",
    " 'https://www.rottentomatoes.com/m/back_to_the_outback']\n",
    "\n",
    "test_links = pickle.load(open( os.path.join('data', MOVIE_LINKS_FILE), 'rb'))[:30]\n",
    "\n",
    "print(\"Number of links\", len(test_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc30e148-c570-4935-b168-5e7a2c611c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6092fd9-6618-4997-ba62-f711d24c4775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cpu count 4\n",
      "First url https://www.rottentomatoes.com/m/the_hating_game\n",
      "First url First urlhttps://www.rottentomatoes.com/m/the_scary_of_sixty_first\n",
      " https://www.rottentomatoes.com/m/rumble_2021\n",
      "i 0\n",
      "Scraping link https://www.rottentomatoes.com/m/the_hating_game\n",
      "i 0\n",
      "Scraping link https://www.rottentomatoes.com/m/rumble_2021\n",
      "i 0\n",
      "Scraping link https://www.rottentomatoes.com/m/the_scary_of_sixty_first\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of errored movie links\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(errored_links))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# print(test_links)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print([links for links in chunks(test_links, 5)])\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrapeMovies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_links\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github-personal/rottentomatoes/.env/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/Documents/github-personal/rottentomatoes/.env/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/Documents/github-personal/rottentomatoes/.env/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/concurrent/futures/_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.5/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "from concurrent import futures\n",
    "from SeleniumDriver import getDriver\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "print('Cpu count', cpu_count())\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield {\"chunk\": lst[i:i + n], \"chunk_number\": i}\n",
    "NOW_DATE_STRING = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "def saveMovies(movies, chunk_number=0): \n",
    "    name_of_file = 'movies-{num_of_movies}-chunk-{chunk_number}-date-{date}.pkl'.format(num_of_movies=len(movies), date=NOW_DATE_STRING, chunk_number=chunk_number)\n",
    "    with open(os.path.join('raw_data', name_of_file), 'wb') as f:\n",
    "            pickle.dump(movies, f)\n",
    "\n",
    "prev_movies_data_hash_map = {}\n",
    "# if (PREV_MOVIES_DATA_FILE):\n",
    "#     prev_movies_data = pickle.load(open(os.path.join('data',PREV_MOVIES_DATA_FILE), 'rb'))\n",
    "#     for movie in prev_movies_data:\n",
    "#         link = movie['link']\n",
    "#         prev_movies_data_hash_map[link] = True\n",
    "\n",
    "def scrapeMovies(test_links, chunk_number):\n",
    "    print('First url' , test_links[0])\n",
    "    driver = getDriver()\n",
    "    scraped_movies = []\n",
    "    errored_links = []\n",
    "\n",
    "    for i in range(len(test_links)):\n",
    "        link = test_links[i]\n",
    "        print(\"i\", i)\n",
    "        # if i > 0 and (i % 500 == 0):\n",
    "        saveMovies(scraped_movies, chunk_number)\n",
    "        try:\n",
    "            if (link in prev_movies_data_hash_map):\n",
    "                continue\n",
    "\n",
    "            print( \"Scraping link\", link)\n",
    "            driver.get(link)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            # TODO Don't skip whole movie if some fields error\n",
    "            score_board_elm = driver.find_element(By.TAG_NAME, \"score-board\")\n",
    "            audience_score = score_board_elm.get_attribute(\"audiencescore\")\n",
    "            tomato_meter_score = score_board_elm.get_attribute(\"tomatometerscore\")\n",
    "            release_date = driver.find_element(By.TAG_NAME, \"time\").text\n",
    "            page_title = driver.title\n",
    "            information = {\n",
    "                \"audience_score\" :  audience_score,\n",
    "                \"tomato_meter_score\" :  tomato_meter_score,\n",
    "                \"release_date\": release_date,\n",
    "                \"page_title\": page_title,\n",
    "                \"link\": link,\n",
    "            }\n",
    "            scraped_movies.append(information)\n",
    "        except Exception as e:\n",
    "            logging.error('bad link', link)\n",
    "            # logging.exception(e, exc_info=True) # Uncomment for debugging, could use debug levels in future\n",
    "            errored_links.append(link)\n",
    "\n",
    "    saveMovies(scraped_movies)\n",
    "    print('Number of errored movie links', len(errored_links))\n",
    "\n",
    "# print(test_links)\n",
    "# print([links for links in chunks(test_links, 5)])\n",
    "Parallel(n_jobs=-1)(delayed(scrapeMovies)(chunk['chunk'], chunk['chunk_number']) for chunk in chunks(test_links,10)) #execute parallel for all urls\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afc7f68d030498b7da4d538db87f988762ac6da8e13974322b0e82e43c04d377"
  },
  "kernelspec": {
   "display_name": "rottentomatoes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
